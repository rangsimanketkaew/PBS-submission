#!/bin/csh

# NWChem Interactive PBS Pro submission program on TAIWANIA cluster, 
# NCHC, Taiwan.
#
# Capabilities:
#  [/] support serial and parallel run using MPI protocol
#  [/] support ARMCI methods: Casper and MPI-PR
#  [/] support GPU/CUDA request
#  [/] automatically determine optimal PBS job queue
#
# Updated 2018.06.29  Rangsiman Ketkaew  rangsiman1993@gmail.com
# https://github.com/rangsimanketkaew/PBS-submission/

###############################################################################
set NWCHEM_VER            = "6.8"
set NWCHEM_TOP            = "/pkg/nwchem/Casper/i18gcc6/nwchem-6.8.1-fixmrcc"
set NWCHEM_TOP_CASPER     = "$NWCHEM_TOP"
set NWCHEM_TOP_MPIPR      = "/pkg/nwchem/MPI-PR/i18gcc6/nwchem-6.8.1-fixmrcc"
set NWCHEM_TOP_CASPER_GPU = "/pkg/nwchem/Casper/i18gcc6/nwchem-6.8.1-fixcuda"
set NWCHEM_TOP_MPIPR_GPU  = "NOTAVAIALBLE"
set NWCHEM_TOP_GPU        = "$NWCHEM_TOP_CASPER_GPU"
###############################################################################
set CLEANSRC              = "$HOME/.nwchem-clean-up"
###############################################################################
set PROJ_ID_FILE          = "$HOME/.proj_id"
###############################################################################

every_thing_starts_from_here:

onintr int

set OUTPUT_ARG2  = 0
set GPU          = 0
set USEGPU       = 0
set ARMCI        = 0
set CASPER       = 0
set CASPER_GPU   = 0
set MPIPR        = 0
set MPIPR_GPU    = 0
set CLEAN_CASPER = 0
set CLEAN_MPIPR  = 0

echo ""
echo "     #################################################################"
echo "    ###   NWChem $NWCHEM_VER Interactive PBS Pro Submission on TAIWANIA   ###"
echo "   #################################################################"
echo ""

#############################################
# Check each argument.
#############################################

# Total arguments
set ARGV_ORDER = "$#argv"

if ($ARGV_ORDER == 0) then
  echo "   Usage: subnwchem input[.nw] [output[.out]] [gpu | casper | mpipr | help]"
  echo ""
  exit 0
else if ("null$1" == "nullhelp" || "null$1" == "null-help" || "null$1" == "null-h") then
  goto help
endif

if ($ARGV_ORDER == 1) then
  set INPUT_ARG1 = "$1"
  set OUTPUT_ARG2 = 0
  goto nwchem_def
endif

if ($ARGV_ORDER >1) then
  set INPUT_ARG1 = "$1"
  @ i = 2
  while ($i <= $ARGV_ORDER)
#############################################
# Check if the 2nd argument is special request 
# (help, gpu, casper, & mpipr).
#############################################
    if ("$i" == "2") then
      set SPEC_RE = `echo $argv[2] | grep -Eiwc 'help|gpu|casper|mpipr'`
      if ($SPEC_RE != "1") then
        set OUTPUT_ARG2 = "$argv[2]"
      else
        goto check_special_argv
      endif
# Check other arguments
    else
check_special_argv:
      set ARGV = $argv[$i]
      if (`echo $ARGV|grep -Eiwc 'help'` == "1") then
        goto help
      else if (`echo $ARGV|grep -Eiwc 'gpu'` == "1") then
        set USEGPU = 1
      else if (`echo $ARGV|grep -Eiwc 'casper'` == "1") then
        set ARMCI = 1
        set CASPER = 1
      else if (`echo $ARGV|grep -Eiwc 'mpipr'` == "1") then
        set ARMCI = 1
        set MPIPR = 1
      else
        echo "Error: Invalid command $ARGV"
        echo -n "######### Enter to start help page. #########" 
        set HELP = "$<"
        clear
        goto help
        exit 1
      endif
    endif
    @ i ++
  end
endif

#############################################
# Check if GPU and/or ARMCI are requested.
#############################################

if ($USEGPU == 1) then
  echo "Enabled NWChem/CUDA"
  if ($ARMCI == 1) then
    goto armci_check
  else
# ARMCU is not used
    set NWCHEM_TOP = "$NWCHEM_TOP_GPU"
# to define $NWCHEM_RUNTIME
    set GPU = 1
    goto start_program
  endif
else
  if ($ARMCI == 1) then
    goto armci_check
  else
    set CASPER = 0
    set MPIPR = 0
    goto nwchem_def
  endif
endif 

#############################################
# If ARMCI is requested, check either Casper
# of MPI-PR will be used.
#############################################

armci_check:
if ( $CASPER == $MPIPR ) then
  echo "Error: Conflict in ARM-CI. Use either Casper or MPI-PR."
  exit 1
endif

if ("null$CASPER" == "null1") then
  set NWCHEM_TOP = "$NWCHEM_TOP_CASPER"
  set CASPER = 1
  set MPIPR = 0
  echo "Enabled ARMCI Casper"
  if ($USEGPU == 1) then
# Set CASPER_GPU = 1 to define $NWCHEM_RUNTIME
    set CASPER_GPU = 1
    set NWCHEM_TOP = "$NWCHEM_TOP_CASPER_GPU"
    goto start_program
  endif
  goto start_program
else if ("null$MPIPR" == "null1") then
  set NWCHEM_TOP = "$NWCHEM_TOP_MPIPR"
  set CASPER = 0
  set MPIPR = 1
  echo "Enabled ARMCI MPI-PR"
  if ($USEGPU == 1) then
# Turn-off NWChem GPU/MPI-PR
    echo "Error: NWChem with GPU & MPI-PR is not available."
# Set MPIPR_GPU = 1 to define $NWCHEM_RUNTIME
    set MPIPR_GPU = 1
    set NWCHEM_TOP = "$NWCHEM_TOP_MPIPR_GPU"
    goto start_program
  endif
  goto start_program
endif

#############################################
# If neither GPU nor ARMCI are requested, 
# Default NWCHEM_TOP is still used.
#############################################

nwchem_def:
set CASPER = 0
set MPIPR = 0

#############################################
# Start program: user is asked to fill the
# necessary information for creating PBS job.
#############################################

start_program:
if (! -e $NWCHEM_TOP) then
  echo 'Error: Unable to locate NWChem directory, $NWCHEM_TOP.'
  echo 'Please set the suitable path of $NWCHEM_TOP at the beginning lines of this program source code'
  exit 1
else
  set NWCHEM_EXE = "$NWCHEM_TOP/bin/LINUX64/nwchem"
endif
if (! -f $NWCHEM_EXE) then
  echo 'Error: Unable to locate "nwchem" executable in $NWCHEM_TOP/bin/LINUX64/ directory.'
  echo 'Please locate the suitable "nwchem" executable, and move to $NWCHEM_TOP/bin/LINUX64/'
  exit 1
endif

#############################################
# Determine Input file.
#############################################

pushd $HOME >& /dev/null
set PWDHOME = `pwd`
popd >& /dev/null
set noglob
set FULLPATH = `pwd | sed -e "s,$PWDHOME,~,"`
unset noglob

#search_input:
#set DEFAULTINPUT = "$FULLPATH/nwchem.nw"
#echo -n "Enter input file [$DEFAULTINPUT]: "
#set JOBINPUT = "$<"
#if ("null$JOBINPUT" == "null") then
#  set JOBINPUT = "$DEFAULTINPUT"
#---------------------------------------------------
##else
##  set JOBINPUT_TYPE = `printf $JOBINPUT | tail -c 3`
##  if ("null$JOBINPUT_TYPE" != "null.nw") then
##    echo "Error: NWChem input file must be *.nw"
##    goto search_input
##  else
##    set INPUTFILE = "$JOBINPUT"
##  endif
#endif
#

set JOBINPUT = "$INPUT_ARG1"
set INPUTFILE = "$FULLPATH/$JOBINPUT"
if (-f $INPUTFILE) then
   set JOBINPUT = "$INPUTFILE"
 else if (-f $INPUTFILE.nw) then
   set JOBINPUT = "$INPUTFILE.nw"
endif
#
if (! -f $JOBINPUT) then
  echo "Error: Failed to locate input file "$JOBINPUT""
  exit 1
endif

#############################################
# Define name of output file.
#############################################

if ($OUTPUT_ARG2 != 0) then
  set OUTPUTFILE = "$OUTPUT_ARG2"
  set OUTPUTNAME = `basename $OUTPUTFILE .out`
  set OUTPUTFILE = "$OUTPUTNAME".out
  set JOBOUTPUT = "$FULLPATH/$OUTPUTFILE"
  goto ask_gpu
endif

set noglob
set JOBOUTPUT  = `basename $JOBINPUT .nw`.out
unset noglob
set DEFAULTOUTPUT = "$JOBOUTPUT"

if (! -e $JOBOUTPUT) then
  set OUTPUTNAME = `basename $JOBINPUT .nw`.out
else
  @ i = 1
  while (-e $JOBOUTPUT)
    echo "@  $JOBOUTPUT  already exists."
    set noglob
    set JOBOUTPUT = `basename $JOBINPUT .nw`.$i.out
    set OUTPUTNAME = `basename $JOBINPUT .nw`.$i.out
    unset noglob
    @ i ++
  end
  set DEFAULTOUTPUT = "$JOBOUTPUT"
endif

#
echo -n "Enter output file [$OUTPUTNAME]: "
set OUTPUTFILE = "$<"
if ("null$OUTPUTFILE" != "null") then
  set noglob
  set JOBOUTPUT = "$FULLPATH/$OUTPUTFILE"
# If user-defined output name is not in .out,
# set foo to foo.out
  set JOBOUTPUT_TYPE = `printf $JOBOUTPUT | tail -c 4`
  if ("null$JOBOUTPUT_TYPE" != "null.out") then
    set JOBOUTPUT = "$FULLPATH/$OUTPUTFILE".out
  endif
  unset noglob
else
  set JOBOUTPUT = "$FULLPATH/$DEFAULTOUTPUT"
endif

#############################################
# Define Computing Resource for GPU queue.
#############################################

ask_gpu:
if ($USEGPU == 1) then
  echo -n "Number of GPU cores [1]: "
  set JOBGPU = "$<"
  if ("null$JOBGPU" == "null") then
   set JOBGPU = 1
  else if ($JOBGPU >= 33) then
   echo "Error: Maximum GPU is 32"
   goto ask_gpu
  else if ($JOBGPU <= 0) then
   echo "Error: Enter only positive integer"
   goto ask_gpu
  endif
endif

# If GPU/CUDA is enabled, skip CPU queue
if ($USEGPU == 1) then
  set TOTALGPU = "$JOBGPU"
  goto gpu_queue
endif

#############################################
# Define Computing Resource for CPU queue.
#############################################

  ask_node:
  echo -n "Number of Compute node [1]: "
  set JOBNODE = "$<"
  if ("null$JOBNODE" == "null") then
   set JOBNODE = 1
  else if ($JOBNODE >= 601) then
   echo "Error: Maximum node is 600"
   goto ask_node
  else if ($JOBNODE <= 0) then
   echo "Error: Enter only positive integer"
   goto ask_node
  endif
#
  ask_cpu:
  echo -n "Number of CPU cores [40]: "
  set JOBCPU = "$<"
  if ("null$JOBCPU" == "null") then
   set JOBCPU = 40
  else if ($JOBCPU >= 41) then
   echo "Error: Maximum CPU cores/node is 40"
   goto ask_cpu
  else if ($JOBCPU <= 0) then
   echo "Error: Enter only positive integer"
   goto ask_cpu
  endif

#############################################
# Number of MPI process is set to = $JOPCPU.
# Each MPI process uses 1 OMP Thread.
# To configure these settings, you must 
# uncomment following lines first.
#############################################

  #ask_mpi:
  #echo -n "Number of MPI process [$JOBCPU]: "
  #set JOBMPI = "$<"
  #if ("null$JOBMPI" == "null") then
   set JOBMPI = $JOBCPU
  #else if ($JOBMPI >= 41) then
  # echo "Error: Maximum MPI process/node is 40"
  # goto ask_mpi
  #else if ($JOBMPI <= 0) then
  # echo "Error: Enter only positive integer"
  # goto ask_mpi
  #endif
#
  #ask_omp:
  #echo -n "Number of OMP Threads [1]: "
  #set JOBOMP = "$<"
  #if ("null$JOBOMP" == "null") then
   set JOBOMP = 1
  #else if ($JOBOMP >= 41) then
  # echo "Error: Maximum OpenMP Threads/node is 40"
  # goto ask_omp
  #else if ($JOBOMP <= 0) then
  # echo "Error: Enter only positive integer"
  # goto ask_omp
  #endif

set TOTALGPU = "-"

#############################################
# Calculate total number of MPI process.
#############################################

@ TOTALMPI = ($JOBNODE * $JOBMPI)
if ( $CASPER != 0 || $MPIPR != 0 ) then
  if ($TOTALMPI == 1) then
    echo "Error: ARMCI Casper are MPI-PR require MPI process > 1"
    goto ask_node
  endif
endif

#############################################
# Search optimal CPU queue and wall-time.
#############################################

cpu_queue:
  if ($TOTALMPI == 1) then
   set CPUQUEUE = "serial"
   set JOBTIME = "96:00:00"
  else if ($TOTALMPI <= 40) then
   set CPUQUEUE = "cf40"
   set JOBTIME = "96:00:00"
  else if ($TOTALMPI <= 160) then
   set CPUQUEUE = "cf160"
   set JOBTIME = "96:00:00"
  else if ($TOTALMPI <= 400) then
   set CPUQUEUE = "ct400"
   set JOBTIME = "96:00:00"
  else if ($TOTALMPI <= 800) then
   set CPUQUEUE = "ct800"
   set JOBTIME = "72:00:00"
  else if ($TOTALMPI <= 1200) then
   set CPUQUEUE = "cf1200"
   set JOBTIME = "48:00:00"
  else if ($TOTALMPI <= 2000) then
   set CPUQUEUE = "ct2k"
   set JOBTIME = "48:00:00"
  else if ($TOTALMPI <= 6000) then
   set CPUQUEUE = "ct6k"
   set JOBTIME = "24:00:00"
  else if ($TOTALMPI <= 8000) then
   set CPUQUEUE = "ct8k"
   set JOBTIME = "24:00:00"
  else if ($TOTALMPI <= 22400) then
   set CPUQUEUE = "ct22400"
   set JOBTIME = "24:00:00"
  endif

check_cpu_queue:
  echo -n "Job Queue [optimal queue is $CPUQUEUE]: "
  set CHECKQUEUE = "$<"
  if ("null$CHECKQUEUE" == "null") then
    set JOBQUEUE = "$CPUQUEUE"
  else if ("null$CHECKQUEUE" == "nullserial") then
    set JOBQUEUE = serial
    goto sum_cpu
  else if ("null$CHECKQUEUE" == "nullctest") then
    set JOBQUEUE = ctest
    set JOBTIME = "00:30:00"
  else
#
    set Q_CPU_LIST = ( serial ctest cf40 cf160 ct400 ct800 cf1200 ct2k ct6k ct8k ct22400 )
    @ i = 1
    while ($i <= $#Q_CPU_LIST)
      if ("$CHECKQUEUE" == "$Q_CPU_LIST[$i]") then
        set TESTQUEUE = 1
        break
      else
        set TESTQUEUE = 0
      endif
      @ i ++
    end
      if ($TESTQUEUE != 1) then
        echo "Error: queue '$CHECKQUEUE' not found."
        echo "Available CPU queues are: $Q_CPU_LIST"
        goto check_cpu_queue
      else
        set JOBQUEUE = "$CHECKQUEUE"
      endif
#
check_cpu_time:
      echo -n "Set wall-time [e.g. 00:30:00]: "
      set JOBTIME = "$<"
      if ("null$JOBTIME" == "null") then
        echo "Error: wall-time is not set yet."
        goto check_cpu_time
      endif
  endif

# Check if requested MPI processes match ctest policy
if ($JOBQUEUE == ctest) then
  if ($TOTALMPI >= 81) then
    echo "Error: total MPI processes you requested, $TOTALMPI processes, violates policy of ctest queue."
    goto check_cpu_queue
  endif
endif

# If CPU queue is used, disabled GPU
sub_cpu:
  set JOBGPU = "-" 
  set SETGPU = ""
  goto job_setting

#############################################
# Search optimal GPU queue and wall-time.
# Previously, $TOTALGPU is set to $JOBCPU.
#############################################

gpu_queue:
  if ($TOTALGPU <= 4) then
   set GPUQUEUE = "gp4"
   set JOBTIME = "96:00:00"
  else if ($TOTALGPU <= 16) then
   set GPUQUEUE = "gp16"
   set JOBTIME = "96:00:00"
  else if ($TOTALGPU <= 32) then
   set GPUQUEUE = "gp32"
   set JOBTIME = "48:00:00"
  endif

#############################################
# Number of Compute node that user can 
# request depends on GPU job queue.
#############################################

# gp4
  if ($GPUQUEUE == gp4) then
    set JOBNODE = 1
# gp16
  else if ($GPUQUEUE == gp16) then
    ask_node_for_gpu:
    echo -n "Number of Compute node [2]: "
    set JOBNODE = "$<"
    if ("null$JOBNODE" == "null") then
     set JOBNODE = 1
    else if ($JOBNODE >= 5) then
     echo "Error: Maximum node is 4"
     goto ask_node_for_gpu
    else if ($JOBNODE <= 0) then
     echo "Error: Enter only positive integer"
     goto ask_node_for_gpu
    else if ($JOBNODE <= 1) then
     echo "Error: Minimum node is 2"
     goto ask_node_for_gpu
    endif
# gp32
  else if ($GPUQUEUE == gp32) then
    ask_node_for_gpu:
    echo -n "Number of Compute node [2]: "
    set JOBNODE = "$<"
    if ("null$JOBNODE" == "null") then
     set JOBNODE = 1
    else if ($JOBNODE >= 9) then
     echo "Error: Maximum node is 8"
     goto ask_node_for_gpu
    else if ($JOBNODE <= 0) then
     echo "Error: Enter only positive integer"
     goto ask_node_for_gpu
    else if ($JOBNODE <= 4) then
     echo "Error: Minimum node is 5"
     goto ask_node_for_gpu
    endif
  endif

  ask_cpu_for_gpu:
  echo -n "Number of CPU cores [40]: "
  set JOBCPU = "$<"
  if ("null$JOBCPU" == "null") then
   set JOBCPU = 40
  else if ($JOBCPU >= 41) then
   echo "Error: Maximum CPU cores/node is 40"
   goto ask_cpu_for_gpu
  else if ($JOBCPU <= 0) then
   echo "Error: Enter only positive integer"
   goto ask_cpu_for_gpu
  endif
## Number of MPI process is set to $JOBCPU.
  set JOBMPI = $JOBCPU
## OMP_NUM_THREADS is set to 1 (per MPI).
  set JOBOMP = 1

#############################################
# Determine total number of MPI process.
#############################################

@ TOTALMPI = ($JOBNODE * $JOBMPI)
if ( $CASPER != 0 || $MPIPR != 0 ) then
  if ($TOTALMPI == 1) then
    echo "Error: ARMCI Casper and MPI-PR require MPI process > 1"
    goto ask_node
  endif
endif

check_gpu_queue:
  echo -n "Job Queue [optimal GPU queue is $GPUQUEUE]: "
  set CHECKQUEUE = "$<"
  if ("null$CHECKQUEUE" == "null") then
    set JOBQUEUE = "$GPUQUEUE"
  else if ("null$CHECKQUEUE" == "nullgtest") then
# 
    ask_gpu_for_gtest:
    if ($USEGPU == 1) then
      echo -n "Number of GPU cores [1]: "
      set JOBGPU = "$<"
      if ("null$JOBGPU" == "null") then
        set JOBGPU = 1
      else if ($JOBGPU >= 9) then
        echo "Error: Maximum GPU is 8"
        goto ask_gpu_for_gtest
      else if ($JOBGPU <= 0) then
        echo "Error: Enter only positive integer"
        goto ask_gpu_for_gtest
      endif
    endif
#
     set TOTALGPU = $JOBGPU
# determine optimal number of cpu
    ask_cpu_for_gtest:
    echo -n "Number of CPU cores [4]: "
    set JOBCPU = "$<"
    if ("null$JOBCPU" == "null") then
      set JOBCPU = 4
    else if ($JOBCPU >= 9) then
      echo "Error: Maximum CPU cores/node is 8"
      goto ask_cpu_for_gtest
    else if ($JOBCPU <= 0) then
      echo "Error: Enter only positive integer"
      goto ask_cpu_for_gtest
    endif
# 
    set JOBQUEUE = gtest
    set JOBTIME = "00:30:00"
    set JOBNODE = 1
    set JOBMPI = $JOBCPU
    set JOBOMP = 1
  else
#
    set Q_GPU_LIST = ( gtest gp4 gp16 gp32 )
    @ i = 1
    while ($i <= $#Q_GPU_LIST)
      if ("$CHECKQUEUE" == "$Q_GPU_LIST[$i]") then
        set TESTQUEUE = 1
        break
      else
        set TESTQUEUE = 0
      endif
      @ i ++
    end
      if ($TESTQUEUE != 1) then
        echo "Error: queue '$CHECKQUEUE' not found."
        echo "Available GPU queues are: $Q_GPU_LIST"
        goto check_gpu_queue
      else
        set JOBQUEUE = "$CHECKQUEUE"
      endif
#
check_gpu_time:
      echo -n "Set wall-time [e.g. 00:30:00]: "
      set JOBTIME = "$<"
      if ("null$JOBTIME" == "null") then
        echo "Error: wall-time is not set yet."
        goto check_gpu_time
      endif
  endif

# Define 'ngpus' for PBS script
  set SETGPU = ":ngpus=$TOTALGPU" 

#############################################
# Define job name & standard error & output 
# and check project ID.
#############################################

job_setting:
set JOBNAME = `basename $JOBINPUT .nw`
set JOBSTDERR = `basename $JOBOUTPUT .out`.stderr
set JOBSTDOUT = `basename $JOBOUTPUT .out`.stdout


#############################################
# Set NWChem runtime and determine clean-up 
# file for Casper and MPI-PR methods.
#############################################

if ("null$GPU" == "null1") then
  set NWCHEM_RUNTIME = GPU
  set NWCHEM_TYPE = "GPU"
else if ("null$CASPER_GPU" == "null1") then
  set NWCHEM_RUNTIME = CASPER_GPU
  set NWCHEM_TYPE = "ARMCI: Casper & GPU"
else if ("null$MPIPR_GPU" == "null1") then
  set NWCHEM_RUNTIME = MPIPR_GPU
  set NWCHEM_TYPE = "ARMCI: MPI-PR & GPU"
else if ("null$CASPER" == "null1") then
  set NWCHEM_RUNTIME = CASPER
  set NWCHEM_TYPE = "ARMCI: Casper"
else if ("null$MPIPR" == "null1") then
  set NWCHEM_RUNTIME = MPIPR
  set NWCHEM_TYPE = "ARMCI: MPI-PR"
else
  set NWCHEM_RUNTIME = NORM
  set NWCHEM_TYPE = "No ARMCI"
endif

#############################################
# Check clean-up file of whether Casper or 
# MPI-PR will be used.
#############################################

if ($NWCHEM_RUNTIME == GPU) then
  set CLEAN_CASPER = 1
else if ($NWCHEM_RUNTIME == CASPER_GPU || $NWCHEM_RUNTIME == CASPER) then
  set CLEAN_CASPER = 1
else if ($NWCHEM_RUNTIME == MPIPR_GPU || $NWCHEM_RUNTIME == MPIPR) then
  set CLEAN_MPIPR = 1
else if ($NWCHEM_RUNTIME == NORM) then
  set CLEAN_CASPER = 1
endif

#############################################
# Check if $CLEANSRC exits
#############################################

if ( ! -e $CLEANSRC ) then
  echo ""
  echo "This may be your first time to use subnwchem. CLEAN-UP file just has been created."
  mkdir -p $CLEANSRC
endif

#--------------------------------------------------------------

clean_casper:
if ( $CLEAN_CASPER == 1 ) then
  set CLEAN_CASPER = "$CLEANSRC/cleanup-devshm.sh"
else
  goto clean_mpipr
endif

if ( ! -f $CLEAN_CASPER ) then
  echo ""
  echo "Error: Failed to locate cleanup-devshm.sh in $CLEANSRC"
  echo ""
  echo -n "Do you want to create cleanup-devshm.sh under your HOME ? [y]: "
  set CREATE_SHM = "$<"
else
  goto job_proj_id
endif

if ( $CREATE_SHM == "" || $CREATE_SHM == "y" || $CREATE_SHM == "yes" ) then
cat <<EOF > $CLEAN_CASPER
#!/bin/bash

######### cleanup-devshm.sh was created by subnwchem #########
# This script will be used to remove garbage on compute nodes 
# before & after running NWChem on TAIWANIA cluster.
##############################################################

MPIPR_JUNK_PREFIX="/dev/shm/*cmx"

GARBAGE=\`ls -1 \${MPIPR_JUNK_PREFIX}* 2>/dev/null\`
if [ "\$GARBAGE" != "" ]; then
  echo "Cleaning up \$GARBAGE at \$HOSTNAME"
  rm -f \$GARBAGE
fi

PSM2_JUNK_PREFIX="/dev/shm/psm2_shm"

GARBAGE=\`ls -1 \${PSM2_JUNK_PREFIX}* 2>/dev/null\`
if [ "\$GARBAGE" != "" ]; then
  echo "Cleaning up \$GARBAGE at \$HOSTNAME"
  rm -f \$GARBAGE
fi

EOF
else if ( $CREATE_SHM == "n" || $CREATE_SHM == "no" )
  exit 0
endif

#--------------------------------------------------------------

clean_mpipr:
if ( $CLEAN_MPIPR == 1 ) then
  set CLEAN_MPIPR = "$CLEANSRC/cleanup-cmx.sh"
else
  goto job_proj_id
endif

if ( ! -f $CLEAN_MPIPR ) then
  echo ""
  echo "Error: Failed to locate cleanup-cmx.sh in $CLEANSRC"
  echo ""
  echo -n "Do you want to create cleanup-cmx under your local directory ? [y]: "
  set CREATE_CMX = "$<"
else
  goto job_proj_id
endif

if ( $CREATE_CMX == "" || $CREATE_CMX == "y" || $CREATE_CMX == "yes" ) then
cat <<EOF > $CLEAN_MPIPR
#!/bin/bash

########### cleanup-cmx.sh was created by subnwchem ##########
# This script will be used to remove garbage on compute nodes 
# before & after running NWChem on TAIWANIA cluster.
##############################################################

MPIPR_JUNK_PREFIX="/dev/shm/*cmx"

GARBAGE=\`ls -1 \${MPIPR_JUNK_PREFIX}* 2>/dev/null\`
if [ "\$GARBAGE" != "" ]; then
  echo "Cleaning up \$GARBAGE at \$HOSTNAME"
  rm -f \$GARBAGE
fi

EOF
else if ( $CREATE_CMX == "n" || $CREATE_CMX == "no" )
  exit 0
endif

#--------------------------------------------------------------

job_proj_id:
set LISTID = ( `get_su_balance | awk -F, '{print $2}' | xargs` )

if ( "$LISTID" == "" ) then
  echo "Error: No available Project ID."
  exit 1
else if ( $#LISTID == 1 ) then
  set PROJ_ID = "$LISTID[1]"
  goto jobinfo
endif

if ( -f $PROJ_ID_FILE ) then
  set PROJID_1 = `head -1 $PROJ_ID_FILE`
  if ( `get_su_balance | grep -wc "$PROJID_1"` == 0 ) then
    echo "Error: Project ID specified in first line of $PROJ_ID_FILE is not correct."
    exit 1
  endif
  set PROJ_ID = "$PROJID_1"
  goto jobinfo
endif

set BALANCE = ( `get_su_balance | awk -F, '{print $1}' | xargs` )
echo ""
echo " Your available Project ID & SU Balance:"
@ i = 1
while ( $i <= $#LISTID )
echo " [$i]  $LISTID[$i]  $BALANCE[$i]"
set PROJ_ID = "$LISTID[1]"
@ i++
end
echo ""

ask_id_choice:
echo -n "Enter Project ID [1]: "
set PROJINP = "$<"

if ( "null$PROJINP" == "null" ) then
  set PROJ_ID = "$LISTID[1]"
  goto jobinfo
endif

if ( `echo $PROJINP | grep -c '[a-z][A-Z]*'` == 1 ) then
  echo "Error: Please assign choice as positive integer."
  goto ask_id_choice
endif

if ( $PROJINP > $#LISTID || $PROJINP <= 0 ) then
  echo "Error: Choice you selected is out of range."
  goto ask_id_choice
else
  set PROJ_ID = "$LISTID[$PROJINP]"
endif

jobinfo:

#############################################
# Show job information before submit
#############################################

  echo ""
  echo " =========================== $ Job Information $ =========================="
  echo " Run on `date` by `whoami`"
  echo ""
  echo " NWChem runtime         = $NWCHEM_VER - $NWCHEM_TYPE"
  echo " NWChem executable      = $NWCHEM_EXE"
  echo ""
  echo " Input file             = $JOBINPUT"
  echo " Output file            = $JOBOUTPUT"
  echo ""
  echo " Compute node           = $JOBNODE"
  echo " CPU cores per node     = $JOBCPU"
  echo " GPU accelerator        = $TOTALGPU"
  echo " MPI process per node   = $JOBMPI"
  echo " OMP Threads per node   = $JOBOMP"
  echo " Total MPI process      = $JOBNODE x $JOBMPI = $TOTALMPI processes"
  echo ""
  echo " Job Name               = $JOBNAME"
  echo " Job Queue              = $JOBQUEUE"
  echo " Wall-time              = $JOBTIME"
  echo " Std Error              = $FULLPATH/$JOBSTDERR"
  echo " Std Output             = $FULLPATH/$JOBSTDOUT"
  echo " Project ID             = $PROJ_ID"
  echo " =========================================================================="
  echo ""

echo -n "Submit your job now ? [yes]: "
set CONFIRM = "$<"

if ("null$CONFIRM" == "null" || "null$CONFIRM" == "nully" || "null$CONFIRM" == "nullyes") then
else
  echo "Error: PBS submission aborted !"
re_submit:
  echo -n "Do you want to re-submit ? [no]: "
  set RESUBMIT = "$<"
    if ($RESUBMIT == "" || $RESUBMIT == "n" || $RESUBMIT == "no") then
      exit 1
    else if ($RESUBMIT == "y" || $RESUBMIT == "yes") then
      goto every_thing_starts_from_here
    else
      echo "Error: [y/n] ?"
      goto re_submit
    endif
endif

#############################################
# Creat PBS script and write all parameter
# and configuration settings.
#############################################

set JOB_SCRIPT = `dirname $JOBOUTPUT`/submit.NWChem.`basename $JOBOUTPUT .out`.sh

cat <<EOF > $JOB_SCRIPT
#!/bin/bash
#PBS -l select=${JOBNODE}:ncpus=${JOBCPU}:mpiprocs=${JOBMPI}:ompthreads=${JOBOMP}$SETGPU
#PBS -l walltime=$JOBTIME
#PBS -q $JOBQUEUE
#PBS -N $JOBNAME
#PBS -e $JOBSTDERR
#PBS -o $JOBSTDOUT
#PBS -P $PROJ_ID

################################################################
#### This PBS Pro script was generated by subnwchem program ####
################################################################

module purge
module load intel/2018_u1 cuda/8.0.61 gcc/6.3.0

cd \$PBS_O_WORKDIR

ulimit -c 0
ulimit -s unlimited

export SCRATCH_DIR=/work1/$USER/SCRATCH/nwchem/nwchem.pbs\${PBS_JOBID/\.srvc1/}
if [ ! -d \$SCRATCH_DIR ]; then mkdir -p \$SCRATCH_DIR; fi

export I_MPI_FABRICS=shm:tmi
export I_MPI_PIN_DOMAIN=omp
export I_MPI_HYDRA_BRANCH_COUNT=-1
export I_MPI_HYDRA_PMI_CONNECT=alltoall

export MPI_ROOT=\$I_MPI_ROOT/intel64
export MPICC=\$MPI_ROOT/bin/mpiicc
export MPICXX=\$MPI_ROOT/bin/mpiicpc
export MPIFC=\$MPI_ROOT/bin/mpiifort
export NWCHEM_CASLIB=$NWCHEM_TOP/../deps/lib/
if [ ! -f ~/.nwchemrc ]; then ln -s /pkg/nwchem/etc/default.nwchemrc ~/.nwchemrc; fi

export MACHLIST=\$PBS_O_WORKDIR/nodelist.\${PBS_JOBID/\.srvc1/}

cat \$PBS_NODEFILE | sed -e 's/.*/&\.nchc\.opa/' > \$MACHLIST
export CASCLEAN="for RUNNODE in \`uniq \$MACHLIST\`; ssh \$RUNNODE $CLEANSRC/cleanup-devshm.sh; done"
export MPIPRCLEAN="for RUNNODE in \`uniq \$MACHLIST\`; ssh \$RUNNODE $CLEANSRC/cleanup-cmx.sh; done"

EOF

#############################################
# Append appropriate commands for cleaning
# and running NWChem with MPI command.
#############################################

if ("null$NWCHEM_RUNTIME" == "nullGPU") then
cat <<EOF >> $JOB_SCRIPT

########################
\$CASCLEAN
########################

mpiexec.hydra -PSM2 -n $TOTALMPI $NWCHEM_EXE \
$JOBINPUT > $JOBOUTPUT

########################
\$CASCLEAN
########################

EOF

#---------------------------------------

else if ("null$NWCHEM_RUNTIME" == "nullCASPER_GPU" || "null$NWCHEM_RUNTIME" == "nullCASPER") then
cat <<EOF >> $JOB_SCRIPT
export ARMCI_NETWORK=ARMCI

########################
\$CASCLEAN
########################

mpiexec.hydra -PSM2 -n $TOTALMPI -genv CSP_NG 1 -genv LD_PRELOAD \$NWCHEM_CASLIB/libcasper.so \
$NWCHEM_EXE \
$JOBINPUT > \
$JOBOUTPUT

########################
\$CASCLEAN
########################

EOF

#---------------------------------------

else if ("null$NWCHEM_RUNTIME" == "nullMPIPR_GPU" || "null$NWCHEM_RUNTIME" == "nullMPIPR") then
cat <<EOF >> $JOB_SCRIPT
export ARMCI_NETWORK=MPI-PR

########################
\$MPIPRCLEAN
########################

mpiexec.hydra -PSM2 -n $TOTALMPI $NWCHEM_EXE \
$JOBINPUT > \
$JOBOUTPUT

########################
\$MPIPRCLEAN
########################

EOF

#---------------------------------------

else if ("null$NWCHEM_RUNTIME" == "nullNORM") then
cat <<EOF >> $JOB_SCRIPT

########################
\$CASCLEAN
########################

mpiexec.hydra -PSM2 -n $TOTALMPI $NWCHEM_EXE \
$JOBINPUT > \
$JOBOUTPUT

########################
\$CASCLEAN
########################

EOF
endif

#############################################
# Submit the job of NWChem using 'qsub'.
#############################################

qsub $JOB_SCRIPT
#  echo "Your job has been submitted."
  exit 0

int:  
 echo ""
 echo "Error: You pressed Ctrl-C  ....quit.... "
 echo ""
 exit 1

help:
clear
cat << EOF | less

 NAME            subnwchem  -  Interactive PBS Professional Job Submission for NWChem $NWCHEM_VER

 SYNOPSIS        subnwchem input[.nw] [output[.out]] [gpu | casper | mpipr | help]

 EXAMPLE         subnwchem water                             Run water.nw and print output to water.out.

                 subnwchem water.nw                          Run water.nw and print output to water.out.

                 subnwchem water water-HF                    Run water.nw and print output to water-HF.out.

                 subnwchem H2.nw mpipr                       Request MPI-PR. \
                                                                              >  Recommended for medium & large jobs.
                 subnwchem H2.nw casper                      Request Casper. /   (Use Casper instead when MPI-PR fails)

                 subnwchem H2.nw gpu                         Request GPU for CUDA run. Job will be submitted on GPU node.
                                                             Output file is H2.out.

                 subnwchem H2.nw gpu mpipr                   [x] Not yet supported.

                 subnwchem H2.nw H2-CCSDT gpu casper         Run H2.nw with CUDA and Casper on GPU node and print output
                                                             to H2-CCSDT.out

 DESCRIPTION     subnwchem is c-shell program script designed to interactively submit NWChem job using PBS Pro scheduler.
                 Note that command line optional argument is case sensitive. Lowercase is only supported.

                 NWChem program package includes several quantum mechanical and quantum molecular dynamics simulations,
                 such as HF, DFT, TD-DFT, PW-DFT, MP2, CI, CC, TCE, MC-SCF, QM/MM, etc.
                 This NWChem can exploit ARMCI method: MPI-PR and Casper, and GPU/CUDA accelerator on GPU compute node.

                 Yet another NWChem runtimes were compiled with ARMCI methods comprise of Casper and MPI-PR models.
                 When ARMCI is enabled, one rank of all MPI ranks on each compute node will be used as ghost processes
                 dedicated to help asynchronous progress for high efficient scalable MPI calculation.
                 For example, request of 40 processes on 20 compute node each, the total MPI processes is 800.
                 If either Casper or MPI-PR is requested, 20 MPI processes are used as ghost (a.k.a. dummy) processes.
                 Hence the total actual MPI processes is 40*20 - 20 = 780. By the way, you can request N+1 MPI processors
                 to compensate the Ghost processor rather N MPI processors.

                 NWChem $NWCHEM_VER also supports General-Purpose Graphical Processing Unit (GP-GPU) and the CUDA platform.
                 GP-GPU/CUDA supports only the tensor contraction engine (TCE) module for coupled-cluster (CC) calculation.
                 Note that CUDA/MPI-PR is not now supported. Please do avoid requesting CUDA and MPI-PR simultaneously.
                 
 COMMANDS
                 input                 -   NWChem input file with or without .nw extension.
                 output                -   NWChem output file with or without .out extension.
                 gpu                   -   Requests GPU accelerator for CUDA to speed up calculation.
                 casper                -   Requests Casper method (against MPI-PR).
                 mpipr                 -   Requests MPI-PR method (against Casper).
                 help                  -   Open this help.

 LIMITATION
                 Maximum compute node  -   600
                 Maximum CPU cores     -   40   (per node)
                 Maximum MPI process   -   40   (per node)
                 Maximum Threads       -   40   (per node & per MPI rank)

 MORE DETAILS    NWChem official website: http://www.nwchem-sw.org 
                 NWChem official manual: https://github.com/nwchemgit/nwchem/wiki

 AUTHOR          Rangsiman Ketkaew  rangsiman1993@gmail.com
                 https://github.com/rangsimanketkaew/PBS-submission

                 Last updated: June 29, 2018

EOF


